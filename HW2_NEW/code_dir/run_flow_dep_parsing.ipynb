{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from utils import get_vocabs_dep_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'data_new'\n",
    "path_train = os.path.join(data_dir, 'train.labeled')\n",
    "path_test = os.path.join(data_dir, 'test.labeled')\n",
    "# path_train = os.path.join(data_dir, 'train_short.labeled')\n",
    "# path_test = os.path.join(data_dir, 'test_short.labeled')\n",
    "\n",
    "# get only train vocabs to know which words are unknown in test\n",
    "paths_list_train = [path_train]\n",
    "word_dict_train, pos_dict_train = get_vocabs_dep_parser(paths_list_train)\n",
    "\n",
    "paths_list_all = [path_train, path_test]\n",
    "word_dict_all, pos_dict_all = get_vocabs_dep_parser(paths_list_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path_train - data_new/train.labeled\n",
      "path_test - data_new/test.labeled\n",
      "idx_pos_mappings - [0, 1, 2, 3, '#', '$', \"''\", '(', ')', ',', '.', ':', 'CC', 'CD', 'DT', 'EX', 'FW', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NN', 'NNP', 'NNPS', 'NNS', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB', '``']\n",
      "pos_idx_mappings - {'<pad>': 0, '<unk>': 1, '<root>': 2, '<root_pos>': 3, '#': 4, '$': 5, \"''\": 6, '(': 7, ')': 8, ',': 9, '.': 10, ':': 11, 'CC': 12, 'CD': 13, 'DT': 14, 'EX': 15, 'FW': 16, 'IN': 17, 'JJ': 18, 'JJR': 19, 'JJS': 20, 'LS': 21, 'MD': 22, 'NN': 23, 'NNP': 24, 'NNPS': 25, 'NNS': 26, 'PDT': 27, 'POS': 28, 'PRP': 29, 'PRP$': 30, 'RB': 31, 'RBR': 32, 'RBS': 33, 'RP': 34, 'SYM': 35, 'TO': 36, 'UH': 37, 'VB': 38, 'VBD': 39, 'VBG': 40, 'VBN': 41, 'VBP': 42, 'VBZ': 43, 'WDT': 44, 'WP': 45, 'WP$': 46, 'WRB': 47, '``': 48}\n",
      "idx_pos_mappings - [0, 1, 2, 3, '#', '$', \"''\", '(', ')', ',', '.', ':', 'CC', 'CD', 'DT', 'EX', 'FW', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NN', 'NNP', 'NNPS', 'NNS', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB', '``']\n",
      "pos_idx_mappings - {'<pad>': 0, '<unk>': 1, '<root>': 2, '<root_pos>': 3, '#': 4, '$': 5, \"''\": 6, '(': 7, ')': 8, ',': 9, '.': 10, ':': 11, 'CC': 12, 'CD': 13, 'DT': 14, 'EX': 15, 'FW': 16, 'IN': 17, 'JJ': 18, 'JJR': 19, 'JJS': 20, 'LS': 21, 'MD': 22, 'NN': 23, 'NNP': 24, 'NNPS': 25, 'NNS': 26, 'PDT': 27, 'POS': 28, 'PRP': 29, 'PRP$': 30, 'RB': 31, 'RBR': 32, 'RBS': 33, 'RP': 34, 'SYM': 35, 'TO': 36, 'UH': 37, 'VB': 38, 'VBD': 39, 'VBG': 40, 'VBN': 41, 'VBP': 42, 'VBZ': 43, 'WDT': 44, 'WP': 45, 'WP$': 46, 'WRB': 47, '``': 48}\n"
     ]
    }
   ],
   "source": [
    "from data_handling import DepDataset\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "print(\"path_train -\", path_train)\n",
    "print(\"path_test -\", path_test)\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "train = DepDataset(word_dict_all, pos_dict_all, data_dir, 'train.labeled', padding=False)\n",
    "train_dataloader = DataLoader(train, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test = DepDataset(word_dict_all, pos_dict_all, data_dir, 'test.labeled', padding=False, train_word_dict=word_dict_train)\n",
    "test_dataloader = DataLoader(test, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# train = DepDataset(word_dict, pos_dict, data_dir, 'train_short', padding=False)\n",
    "# train_dataloader = DataLoader(train, batch_size=BATCH_SIZE, shuffle=True)\n",
    "# test = DepDataset(word_dict, pos_dict, data_dir, 'test_short', padding=False)\n",
    "# test_dataloader = DataLoader(test, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Train Sentences  5000\n",
      "Number of Test Sentences  1000\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of Train Sentences \", len(train))\n",
    "print(\"Number of Test Sentences \",len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "unknown accuracy: 88.81560287464347\n",
      "Epoch: 1 | Training Loss: 1.1481 | Training accuracy: 65.920% | Test Loss: 0.4553 | Test accuracy: 86.593% | Epoch Time: 43.95 secs\n",
      "saving model\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "unknown accuracy: 91.58636088525388\n",
      "Epoch: 2 | Training Loss: 0.3616 | Training accuracy: 89.196% | Test Loss: 0.3756 | Test accuracy: 88.604% | Epoch Time: 38.81 secs\n",
      "saving model\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "unknown accuracy: 92.36756512586771\n",
      "Epoch: 3 | Training Loss: 0.2340 | Training accuracy: 92.791% | Test Loss: 0.3741 | Test accuracy: 89.357% | Epoch Time: 38.83 secs\n",
      "saving model\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from model import DnnSepParser\n",
    "from trainer import Trainer\n",
    "from loss import NllLoss\n",
    "from utils import IGNORE_IDX\n",
    "\n",
    "\n",
    "WORD_EMBEDDING_DIM = 100\n",
    "TAG_EMBEDDING_DIM = 25\n",
    "word_vocab_size = len(train.word_idx_mappings)\n",
    "tag_vocab_size = len(train.pos_idx_mappings)\n",
    "max_sentence_len = max(train.max_seq_len, test.max_seq_len)\n",
    "ACCUMULATE_GRAD_STEPS_LIST = [50]\n",
    "NUM_EPOCHS = 30\n",
    "len_train = len(train)\n",
    "len_test = len(test)\n",
    "num_layer=2\n",
    "torch.manual_seed(0)\n",
    "for acumulate_grad_steps in ACCUMULATE_GRAD_STEPS_LIST:\n",
    "    model = DnnSepParser(WORD_EMBEDDING_DIM, TAG_EMBEDDING_DIM, num_layer, word_vocab_size, tag_vocab_size, hidden_fc_dim=100)\n",
    "\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "\n",
    "    if use_cuda:\n",
    "        model.cuda()\n",
    "\n",
    "    # TODO: use our function with ignore index\n",
    "#     loss_function = nn.NLLLoss(ignore_index=IGNORE_IDX)\n",
    "#     loss_function = nn.NLLLoss()\n",
    "    loss_function = NllLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "    trainer = Trainer(model, optimizer, loss_function, device)\n",
    "\n",
    "    trainer.train_dep_parser(NUM_EPOCHS, train_dataloader, test_dataloader, acumulate_grad_steps, len_train, len_test, early_stopping=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
