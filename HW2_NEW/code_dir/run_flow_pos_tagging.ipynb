{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from utils import get_vocabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "data_dir = 'data_old'\n",
    "# path_train = os.path.join(data_dir, 'train1_short.wtag')\n",
    "# path_test = os.path.join(data_dir, 'test1_short.wtag')\n",
    "path_train = os.path.join(data_dir, 'train.wtag')\n",
    "path_test = os.path.join(data_dir, 'test.wtag')\n",
    "paths_list = [path_train, path_test]\n",
    "word_dict, pos_dict = get_vocabs(paths_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path_train - data_old/train.wtag\n",
      "path_test - data_old/test.wtag\n",
      "idx_pos_mappings - [0, 1, '#', '$', \"''\", ',', '-LRB-', '-RRB-', '.', ':', 'CC', 'CD', 'DT', 'EX', 'FW', 'IN', 'JJ', 'JJR', 'JJS', 'MD', 'NN', 'NNP', 'NNPS', 'NNS', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB', '``']\n",
      "pos_idx_mappings - {'<pad>': 0, '<unk>': 1, '#': 2, '$': 3, \"''\": 4, ',': 5, '-LRB-': 6, '-RRB-': 7, '.': 8, ':': 9, 'CC': 10, 'CD': 11, 'DT': 12, 'EX': 13, 'FW': 14, 'IN': 15, 'JJ': 16, 'JJR': 17, 'JJS': 18, 'MD': 19, 'NN': 20, 'NNP': 21, 'NNPS': 22, 'NNS': 23, 'PDT': 24, 'POS': 25, 'PRP': 26, 'PRP$': 27, 'RB': 28, 'RBR': 29, 'RBS': 30, 'RP': 31, 'SYM': 32, 'TO': 33, 'UH': 34, 'VB': 35, 'VBD': 36, 'VBG': 37, 'VBN': 38, 'VBP': 39, 'VBZ': 40, 'WDT': 41, 'WP': 42, 'WP$': 43, 'WRB': 44, '``': 45}\n",
      "idx_pos_mappings - [0, 1, '#', '$', \"''\", ',', '-LRB-', '-RRB-', '.', ':', 'CC', 'CD', 'DT', 'EX', 'FW', 'IN', 'JJ', 'JJR', 'JJS', 'MD', 'NN', 'NNP', 'NNPS', 'NNS', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB', '``']\n",
      "pos_idx_mappings - {'<pad>': 0, '<unk>': 1, '#': 2, '$': 3, \"''\": 4, ',': 5, '-LRB-': 6, '-RRB-': 7, '.': 8, ':': 9, 'CC': 10, 'CD': 11, 'DT': 12, 'EX': 13, 'FW': 14, 'IN': 15, 'JJ': 16, 'JJR': 17, 'JJS': 18, 'MD': 19, 'NN': 20, 'NNP': 21, 'NNPS': 22, 'NNS': 23, 'PDT': 24, 'POS': 25, 'PRP': 26, 'PRP$': 27, 'RB': 28, 'RBR': 29, 'RBS': 30, 'RP': 31, 'SYM': 32, 'TO': 33, 'UH': 34, 'VB': 35, 'VBD': 36, 'VBG': 37, 'VBN': 38, 'VBP': 39, 'VBZ': 40, 'WDT': 41, 'WP': 42, 'WP$': 43, 'WRB': 44, '``': 45}\n"
     ]
    }
   ],
   "source": [
    "from data_handling import PosDataset\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "print(\"path_train -\", path_train)\n",
    "print(\"path_test -\", path_test)\n",
    "BATCH_SIZE = 1\n",
    "paths_list = [path_train, path_test]\n",
    "word_dict, pos_dict = get_vocabs(paths_list)\n",
    "train = PosDataset(word_dict, pos_dict, data_dir, 'train', padding=False)\n",
    "train_dataloader = DataLoader(train, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test = PosDataset(word_dict, pos_dict, data_dir, 'test', padding=False)\n",
    "test_dataloader = DataLoader(test, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Train Tagged Sentences  5000\n",
      "Number of Test Tagged Sentences  1000\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of Train Tagged Sentences \", len(train))\n",
    "print(\"Number of Test Tagged Sentences \",len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word embeddings shape: torch.Size([15212, 300])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from model import DnnPosTagger\n",
    "from trainer import Trainer\n",
    "\n",
    "WORD_EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 100\n",
    "NUM_LAYERS = 2\n",
    "word_vocab_size = len(train.word_idx_mappings)\n",
    "tag_vocab_size = len(train.pos_idx_mappings)\n",
    "\n",
    "model = DnnPosTagger(train_dataloader.dataset.word_vectors, HIDDEN_DIM, NUM_LAYERS, word_vocab_size, tag_vocab_size)\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "\n",
    "if use_cuda:\n",
    "    model.cuda()\n",
    "    \n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "acumulate_grad_steps = 50 # This is the actual batch_size, while we officially use batch_size=1\n",
    "trainer = Trainer(model, optimizer, loss_function, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/student/hw2/code_dir/trainer.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  acc += torch.mean(torch.tensor(pos_idx_tensor.to(\"cpu\") == indices.to(\"cpu\"), dtype=torch.float))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Loss: 0.0137 | Training accuracy: 97.065% | Test accuracy: 94.506% | Epoch Time: 36.19 secs\n",
      "saving model\n",
      "Epoch: 2 | Loss: 0.0020 | Training accuracy: 98.391% | Test accuracy: 94.846% | Epoch Time: 34.54 secs\n",
      "saving model\n",
      "Epoch: 3 | Loss: 0.0011 | Training accuracy: 98.910% | Test accuracy: 94.753% | Epoch Time: 34.66 secs\n",
      "Epoch: 4 | Loss: 0.0007 | Training accuracy: 99.161% | Test accuracy: 94.789% | Epoch Time: 35.31 secs\n",
      "Epoch: 5 | Loss: 0.0005 | Training accuracy: 99.519% | Test accuracy: 95.005% | Epoch Time: 34.94 secs\n",
      "saving model\n",
      "Epoch: 6 | Loss: 0.0003 | Training accuracy: 99.691% | Test accuracy: 94.868% | Epoch Time: 35.53 secs\n",
      "Epoch: 7 | Loss: 0.0002 | Training accuracy: 99.802% | Test accuracy: 94.991% | Epoch Time: 34.59 secs\n",
      "Epoch: 8 | Loss: 0.0002 | Training accuracy: 99.803% | Test accuracy: 94.757% | Epoch Time: 34.80 secs\n",
      "Epoch: 9 | Loss: 0.0001 | Training accuracy: 99.858% | Test accuracy: 94.775% | Epoch Time: 36.02 secs\n",
      "Epoch: 10 | Loss: 0.0001 | Training accuracy: 99.873% | Test accuracy: 94.642% | Epoch Time: 35.60 secs\n",
      "Epoch: 11 | Loss: 0.0002 | Training accuracy: 99.791% | Test accuracy: 94.536% | Epoch Time: 35.78 secs\n",
      "Epoch: 12 | Loss: 0.0003 | Training accuracy: 99.391% | Test accuracy: 94.296% | Epoch Time: 34.91 secs\n",
      "Epoch: 13 | Loss: 0.0007 | Training accuracy: 98.892% | Test accuracy: 93.472% | Epoch Time: 34.83 secs\n",
      "Epoch: 14 | Loss: 0.0009 | Training accuracy: 98.930% | Test accuracy: 93.864% | Epoch Time: 34.50 secs\n",
      "Epoch: 15 | Loss: 0.0008 | Training accuracy: 99.294% | Test accuracy: 93.874% | Epoch Time: 34.85 secs\n",
      "==> Finished Training ...\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 15\n",
    "acumulate_grad_steps = 50\n",
    "len_train = len(train)\n",
    "len_test = len(test)\n",
    "trainer.train(EPOCHS, train_dataloader, test_dataloader, acumulate_grad_steps, len_train, len_test, early_stopping=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
