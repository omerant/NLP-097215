{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fdd3a9313b0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from utils import get_vocabs_dep_parser\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'data_new'\n",
    "path_train = os.path.join(data_dir, 'train.labeled')\n",
    "path_test = os.path.join(data_dir, 'test.labeled')\n",
    "paths_list = [path_train, path_test]\n",
    "word_dict, pos_dict = get_vocabs_dep_parser(paths_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path_train - data_new/train.labeled\n",
      "path_test - data_new/test.labeled\n",
      "idx_pos_mappings - [0, 1, 2, '#', '$', \"''\", '(', ')', ',', '.', ':', 'CC', 'CD', 'DT', 'EX', 'FW', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NN', 'NNP', 'NNPS', 'NNS', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB', '``']\n",
      "pos_idx_mappings - {'<pad>': 0, '<unk>': 1, '<root>': 2, '#': 3, '$': 4, \"''\": 5, '(': 6, ')': 7, ',': 8, '.': 9, ':': 10, 'CC': 11, 'CD': 12, 'DT': 13, 'EX': 14, 'FW': 15, 'IN': 16, 'JJ': 17, 'JJR': 18, 'JJS': 19, 'LS': 20, 'MD': 21, 'NN': 22, 'NNP': 23, 'NNPS': 24, 'NNS': 25, 'PDT': 26, 'POS': 27, 'PRP': 28, 'PRP$': 29, 'RB': 30, 'RBR': 31, 'RBS': 32, 'RP': 33, 'SYM': 34, 'TO': 35, 'UH': 36, 'VB': 37, 'VBD': 38, 'VBG': 39, 'VBN': 40, 'VBP': 41, 'VBZ': 42, 'WDT': 43, 'WP': 44, 'WP$': 45, 'WRB': 46, '``': 47}\n",
      "idx_pos_mappings - [0, 1, 2, '#', '$', \"''\", '(', ')', ',', '.', ':', 'CC', 'CD', 'DT', 'EX', 'FW', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NN', 'NNP', 'NNPS', 'NNS', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB', '``']\n",
      "pos_idx_mappings - {'<pad>': 0, '<unk>': 1, '<root>': 2, '#': 3, '$': 4, \"''\": 5, '(': 6, ')': 7, ',': 8, '.': 9, ':': 10, 'CC': 11, 'CD': 12, 'DT': 13, 'EX': 14, 'FW': 15, 'IN': 16, 'JJ': 17, 'JJR': 18, 'JJS': 19, 'LS': 20, 'MD': 21, 'NN': 22, 'NNP': 23, 'NNPS': 24, 'NNS': 25, 'PDT': 26, 'POS': 27, 'PRP': 28, 'PRP$': 29, 'RB': 30, 'RBR': 31, 'RBS': 32, 'RP': 33, 'SYM': 34, 'TO': 35, 'UH': 36, 'VB': 37, 'VBD': 38, 'VBG': 39, 'VBN': 40, 'VBP': 41, 'VBZ': 42, 'WDT': 43, 'WP': 44, 'WP$': 45, 'WRB': 46, '``': 47}\n"
     ]
    }
   ],
   "source": [
    "from data_handling import DepDataset\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "print(\"path_train -\", path_train)\n",
    "print(\"path_test -\", path_test)\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "train = DepDataset(word_dict, pos_dict, data_dir, 'train', padding=False)\n",
    "train_dataloader = DataLoader(train, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test = DepDataset(word_dict, pos_dict, data_dir, 'test', padding=False)\n",
    "test_dataloader = DataLoader(test, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Train Sentences  5000\n",
      "Number of Test Sentences  1000\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of Train Sentences \", len(train))\n",
    "print(\"Number of Test Sentences \",len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from model import DnnSepParser\n",
    "from trainer import Trainer\n",
    "from loss import NllLoss\n",
    "from utils import IGNORE_IDX\n",
    "\n",
    "WORD_EMBEDDING_DIM = 100\n",
    "TAG_EMBEDDING_DIM = 25\n",
    "NUM_LAYERS = 2\n",
    "word_vocab_size = len(train.word_idx_mappings)\n",
    "tag_vocab_size = len(train.pos_idx_mappings)\n",
    "max_sentence_len = max(train.max_seq_len, test.max_seq_len)\n",
    "model = DnnSepParser(WORD_EMBEDDING_DIM, TAG_EMBEDDING_DIM, NUM_LAYERS, word_vocab_size, tag_vocab_size, max_sentence_len)\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "\n",
    "if use_cuda:\n",
    "    model.cuda()\n",
    "\n",
    "#TODO: use our function with ignore index\n",
    "# loss_function = nn.NLLLoss(ignore_index=IGNORE_IDX)\n",
    "loss_function = nn.NLLLoss()\n",
    "# loss_function = NllLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "acumulate_grad_steps = 50 # This is the actual batch_size, while we officially use batch_size=1\n",
    "trainer = Trainer(model, optimizer, loss_function, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Training Loss: 0.0670 | Training accuracy: 10.046% | Test Loss: 0.0647 | Test accuracy: 9.546% | Epoch Time: 33.54 secs\n",
      "saving model\n",
      "Epoch: 2 | Training Loss: 0.0638 | Training accuracy: 11.539% | Test Loss: 0.0648 | Test accuracy: 9.874% | Epoch Time: 39.06 secs\n",
      "saving model\n",
      "Epoch: 3 | Training Loss: 0.0628 | Training accuracy: 12.630% | Test Loss: 0.0649 | Test accuracy: 10.045% | Epoch Time: 33.96 secs\n",
      "saving model\n",
      "Epoch: 4 | Training Loss: 0.0618 | Training accuracy: 14.113% | Test Loss: 0.0657 | Test accuracy: 10.039% | Epoch Time: 32.97 secs\n",
      "Epoch: 5 | Training Loss: 0.0606 | Training accuracy: 15.257% | Test Loss: 0.0664 | Test accuracy: 10.133% | Epoch Time: 35.20 secs\n",
      "saving model\n",
      "Epoch: 6 | Training Loss: 0.0594 | Training accuracy: 17.058% | Test Loss: 0.0680 | Test accuracy: 9.899% | Epoch Time: 32.69 secs\n",
      "Epoch: 7 | Training Loss: 0.0582 | Training accuracy: 17.766% | Test Loss: 0.0696 | Test accuracy: 9.690% | Epoch Time: 32.80 secs\n",
      "Epoch: 8 | Training Loss: 0.0571 | Training accuracy: 18.648% | Test Loss: 0.0705 | Test accuracy: 9.904% | Epoch Time: 32.93 secs\n",
      "Epoch: 9 | Training Loss: 0.0562 | Training accuracy: 19.507% | Test Loss: 0.0717 | Test accuracy: 9.959% | Epoch Time: 32.82 secs\n",
      "Epoch: 10 | Training Loss: 0.0554 | Training accuracy: 19.797% | Test Loss: 0.0723 | Test accuracy: 10.102% | Epoch Time: 32.95 secs\n",
      "Epoch: 11 | Training Loss: 0.0547 | Training accuracy: 20.203% | Test Loss: 0.0738 | Test accuracy: 10.024% | Epoch Time: 32.47 secs\n",
      "Epoch: 12 | Training Loss: 0.0542 | Training accuracy: 20.465% | Test Loss: 0.0745 | Test accuracy: 9.877% | Epoch Time: 32.37 secs\n",
      "Epoch: 13 | Training Loss: 0.0538 | Training accuracy: 20.140% | Test Loss: 0.0750 | Test accuracy: 9.630% | Epoch Time: 32.97 secs\n",
      "Epoch: 14 | Training Loss: 0.0534 | Training accuracy: 20.308% | Test Loss: 0.0752 | Test accuracy: 9.668% | Epoch Time: 33.23 secs\n",
      "Epoch: 15 | Training Loss: 0.0531 | Training accuracy: 20.762% | Test Loss: 0.0760 | Test accuracy: 9.665% | Epoch Time: 32.78 secs\n",
      "Epoch: 16 | Training Loss: 0.0528 | Training accuracy: 20.529% | Test Loss: 0.0769 | Test accuracy: 9.768% | Epoch Time: 32.66 secs\n",
      "Epoch: 17 | Training Loss: 0.0525 | Training accuracy: 20.372% | Test Loss: 0.0771 | Test accuracy: 9.529% | Epoch Time: 32.88 secs\n",
      "Epoch: 18 | Training Loss: 0.0522 | Training accuracy: 20.866% | Test Loss: 0.0774 | Test accuracy: 9.989% | Epoch Time: 32.96 secs\n",
      "Epoch: 19 | Training Loss: 0.0521 | Training accuracy: 20.750% | Test Loss: 0.0780 | Test accuracy: 9.935% | Epoch Time: 32.69 secs\n",
      "Epoch: 20 | Training Loss: 0.0519 | Training accuracy: 20.875% | Test Loss: 0.0780 | Test accuracy: 9.984% | Epoch Time: 31.68 secs\n",
      "Epoch: 21 | Training Loss: 0.0518 | Training accuracy: 20.792% | Test Loss: 0.0784 | Test accuracy: 9.802% | Epoch Time: 33.90 secs\n",
      "Epoch: 22 | Training Loss: 0.0516 | Training accuracy: 20.685% | Test Loss: 0.0790 | Test accuracy: 9.526% | Epoch Time: 32.50 secs\n",
      "Epoch: 23 | Training Loss: 0.0515 | Training accuracy: 20.938% | Test Loss: 0.0791 | Test accuracy: 9.933% | Epoch Time: 32.49 secs\n",
      "Epoch: 24 | Training Loss: 0.0514 | Training accuracy: 20.821% | Test Loss: 0.0789 | Test accuracy: 10.125% | Epoch Time: 31.98 secs\n",
      "Epoch: 25 | Training Loss: 0.0513 | Training accuracy: 20.720% | Test Loss: 0.0797 | Test accuracy: 9.808% | Epoch Time: 33.50 secs\n",
      "Epoch: 26 | Training Loss: 0.0511 | Training accuracy: 21.048% | Test Loss: 0.0797 | Test accuracy: 10.019% | Epoch Time: 32.77 secs\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "EPOCHS = 100\n",
    "acumulate_grad_steps = 50\n",
    "len_train = len(train)\n",
    "len_test = len(test)\n",
    "trainer.train_dep_parser(EPOCHS, train_dataloader, test_dataloader, acumulate_grad_steps, len_train, len_test, early_stopping=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-nlp_hw2] *",
   "language": "python",
   "name": "conda-env-.conda-nlp_hw2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
